# -*- coding: utf-8 -*-
"""TinyMachineLearning_ESP32-CAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10yAlkIaKT-8c26UK0aejPtyCC5eGB23i
"""

import tensorflow as tf
import numpy as np
import os
from tensorflow.keras import layers, models

# --- 1. Definición de Parámetros (Ajustados a tu Roadmap) ---
IMG_HEIGHT = 64
IMG_WIDTH = 64
IMG_CHANELS = 1 # Monocromo
BATCH_SIZE = 32
# Rutas (asumiendo que 'carpeta_entrenamiento' y 'carpeta_validacion' están definidas)
# ...

# --- 2. Carga de Datos Eficiente (El método recomendado) ---

# Carga los datos pidiendo 64x64 y monocromático directamente
train_ds = tf.keras.utils.image_dataset_from_directory(
    carpeta_entrenamiento,
    labels='inferred',
    label_mode='binary', # 'binary' es para Sigmoid (etiquetas 0 o 1)
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    shuffle=True
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    carpeta_validacion,
    labels='inferred',
    label_mode='binary',
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    color_mode='grayscale',
    batch_size=BATCH_SIZE,
    shuffle=False
)

# Optimiza la carga
train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)


# --- 3. Definición del Modelo ---

data_argumentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
], name="data_argumentation")

model = models.Sequential([
    # Capa de entrada. Especifica la forma Y normaliza los datos.
    layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANELS)),
    layers.Rescaling(1./255), # Capa de normalización

    # Capa Convolucional pequeña
    layers.Conv2D(8, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),

    # Segunda capa opcional (ayuda a la precisión sin añadir mucho tamaño)
    layers.Conv2D(16, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),

    layers.Flatten(),

    layers.Dropout(0.5),
    # Capa Densa oculta pequeña
    layers.Dense(16, activation='relu'),

    # Capa de Salida (1 neurona, Sigmoid para binario)
    layers.Dense(1, activation='sigmoid') # <-- ¡TU PETICIÓN ORIGINAL!
])

model.summary()

# --- 4. Compilación del Modelo ---
model.compile(optimizer='adam',
              loss='binary_crossentropy', # <-- ¡La pérdida correcta para Sigmoid!
              metrics=['accuracy'])

#Define el "calback" de parada temprana
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', #Monitorea la pérdida de validación
    patience=5, #Espera 5 épocas antes de parar
    restore_best_weights=True #Guarda el mejor modelo, no el último
)

# --- 5. Entrenamiento ---

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Entrena el modelo
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50,
    callbacks=[early_stopping]
)

import tensorflow_datasets as tfds
#Carga de dataset 'cats_vs_dogs
# 'Split' divide los datos: 89% para entrenamiento , 10% para validación,
#10% de prueba

(ds_train, ds_val, ds_test), ds_info = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info= True, #Para obtener información del dataset
    as_supervised=True, #Devuelve (imagen, etiqueta) en tuplas
)

print(f"Total de imagenes de entrenamiento: {tfds.experimental.cardinality(ds_train)}")

import os
import tensorflow as tf # Ensure tf is imported for tf.keras.utils.get_file

print("Descargando ZIP de datos")
url = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'
# tf.keras.utils.get_file extracts the content into a temporary directory.
# This function returns the path to that temporary extraction directory.
path_to_zip_extraction_root = tf.keras.utils.get_file('cats_and_dogs_filtered.zip', origin=url, extract=True)

# Variables para rutas en disco
# The actual dataset content (train/validation folders) is inside 'cats_and_dogs_filtered'
# which itself is inside the 'path_to_zip_extraction_root'.
carpeta_base = os.path.join(path_to_zip_extraction_root, 'cats_and_dogs_filtered')
carpeta_entrenamiento = os.path.join(carpeta_base, 'train')
carpeta_validacion = os.path.join(carpeta_base, 'validation')

carp_entren_gatos = os.path.join(carpeta_entrenamiento, 'cats')  # imagenes de gatos para pruebas
carpeta_entren_perros = os.path.join(carpeta_entrenamiento, 'dogs')  # imagenes de perros para pruebas
carpeta_val_gatos = os.path.join(carpeta_validacion, 'cats')  # imagenes de gatos para validacion
carpeta_val_perros = os.path.join(carpeta_validacion, 'dogs')  # imagenes de perros para validacion

pip install tensorflow